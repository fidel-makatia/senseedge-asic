# SPDX-License-Identifier: Apache-2.0
# SenseEdge Weight Export Tool
# Converts trained numpy weights (.npz) to C header for firmware loading

"""
Export INT8 neural network weights to a C header file for the SenseEdge ASIC.

Reads the .npz file produced by train_senseedge.py and generates
firmware/nn_weights.h with arrays matching the nn_engine.v memory layout:

  [0..127]   Layer 1 weights  (16 neurons x 8 inputs, row-major)
  [128..143] Layer 1 biases   (16 values)
  [144..207] Layer 2 weights  (4 neurons x 16 inputs, row-major)
  [208..211] Layer 2 biases   (4 values)
  ---------
  212 total INT8 parameters
"""

import argparse
import os
import sys
import numpy as np


def load_weights(npz_path):
    """Load the .npz weight file and return individual arrays.

    Returns:
        layer1_weights : (16, 8)  int8
        layer1_biases  : (16,)    int8
        layer2_weights : (4, 16)  int8
        layer2_biases  : (4,)     int8
    """
    data = np.load(npz_path)

    l1w = data["layer1_weights"]
    l1b = data["layer1_biases"]
    l2w = data["layer2_weights"]
    l2b = data["layer2_biases"]

    # Validate shapes
    assert l1w.shape == (16, 8),  f"layer1_weights: expected (16,8), got {l1w.shape}"
    assert l1b.shape == (16,),    f"layer1_biases: expected (16,), got {l1b.shape}"
    assert l2w.shape == (4, 16),  f"layer2_weights: expected (4,16), got {l2w.shape}"
    assert l2b.shape == (4,),     f"layer2_biases: expected (4,), got {l2b.shape}"

    total = l1w.size + l1b.size + l2w.size + l2b.size
    assert total == 212, f"Expected 212 parameters, got {total}"

    return l1w, l1b, l2w, l2b


def format_int8_array(arr, name, indent="    "):
    """Format an int8 array as a C initializer list."""
    flat = arr.flatten().astype(np.int64)
    lines = []
    # 16 values per line for readability
    vals_per_line = 16
    for i in range(0, len(flat), vals_per_line):
        chunk = flat[i:i + vals_per_line]
        line = ", ".join(f"{v:4d}" for v in chunk)
        lines.append(f"{indent}{line}")
    return ",\n".join(lines)


def generate_header(l1w, l1b, l2w, l2b):
    """Generate the C header file contents as a string."""
    # Build the flat all_weights array matching hardware memory layout
    all_weights = np.concatenate([
        l1w.flatten(), l1b.flatten(),
        l2w.flatten(), l2b.flatten()
    ]).astype(np.int8)

    header = []
    header.append("// SPDX-License-Identifier: Apache-2.0")
    header.append("// SenseEdge Neural Network Weights (INT8)")
    header.append("// Auto-generated by ml/export_weights.py -- do not edit by hand")
    header.append("//")
    header.append("// Network: 8 inputs -> 16 hidden (ReLU) -> 4 outputs (argmax)")
    header.append("// Total parameters: 212")
    header.append("//")
    header.append("// Memory layout (matches nn_engine.v):")
    header.append("//   [  0..127] Layer 1 weights  (16 neurons x 8 inputs, row-major)")
    header.append("//   [128..143] Layer 1 biases   (16 values)")
    header.append("//   [144..207] Layer 2 weights  (4 neurons x 16 inputs, row-major)")
    header.append("//   [208..211] Layer 2 biases   (4 values)")
    header.append("//")
    header.append("// Load into hardware via Wishbone register SE_NN_WEIGHTS:")
    header.append("//   for (int i = 0; i < 212; i++)")
    header.append("//       reg_write(SE_NN_WEIGHTS, NN_WEIGHT(i, all_weights[i]));")
    header.append("")
    header.append("#ifndef NN_WEIGHTS_H")
    header.append("#define NN_WEIGHTS_H")
    header.append("")
    header.append("#include <stdint.h>")
    header.append("")

    # --- Layer 1 weights: 16 neurons x 8 inputs ---
    header.append("// Layer 1 weights: 16 neurons x 8 inputs (row-major)")
    header.append("// Address range: [0..127]")
    header.append("// Weight for neuron n, input i = layer1_weights[n * 8 + i]")
    header.append("static const int8_t layer1_weights[128] = {")
    for n in range(16):
        row = l1w[n]
        vals = ", ".join(f"{int(v):4d}" for v in row)
        header.append(f"    {vals},  // neuron {n:2d}")
    header.append("};")
    header.append("")

    # --- Layer 1 biases ---
    header.append("// Layer 1 biases: 16 values")
    header.append("// Address range: [128..143]")
    header.append("static const int8_t layer1_biases[16] = {")
    vals = ", ".join(f"{int(v):4d}" for v in l1b)
    header.append(f"    {vals}")
    header.append("};")
    header.append("")

    # --- Layer 2 weights: 4 neurons x 16 inputs ---
    header.append("// Layer 2 weights: 4 neurons x 16 inputs (row-major)")
    header.append("// Address range: [144..207]")
    header.append("// Weight for neuron n, input i = layer2_weights[n * 16 + i]")
    header.append("static const int8_t layer2_weights[64] = {")
    for n in range(4):
        row = l2w[n]
        vals = ", ".join(f"{int(v):4d}" for v in row)
        header.append(f"    {vals},  // neuron {n}")
    header.append("};")
    header.append("")

    # --- Layer 2 biases ---
    header.append("// Layer 2 biases: 4 values")
    header.append("// Address range: [208..211]")
    header.append("static const int8_t layer2_biases[4] = {")
    vals = ", ".join(f"{int(v):4d}" for v in l2b)
    header.append(f"    {vals}")
    header.append("};")
    header.append("")

    # --- Flat all_weights array for sequential loading ---
    header.append("// All 212 parameters in flat array for sequential weight loading")
    header.append("// Layout: L1 weights[128] | L1 biases[16] | L2 weights[64] | L2 biases[4]")
    header.append("// Load via: for (int i = 0; i < 212; i++)")
    header.append("//               reg_write(SE_NN_WEIGHTS, NN_WEIGHT(i, all_weights[i]));")
    header.append("static const int8_t all_weights[212] = {")

    # Format with section comments
    flat = all_weights.astype(np.int64)
    sections = [
        (0,   128, "Layer 1 weights [0..127]"),
        (128, 144, "Layer 1 biases [128..143]"),
        (144, 208, "Layer 2 weights [144..207]"),
        (208, 212, "Layer 2 biases [208..211]"),
    ]
    for start, end, comment in sections:
        header.append(f"    // {comment}")
        chunk = flat[start:end]
        vals_per_line = 16
        for i in range(0, len(chunk), vals_per_line):
            row = chunk[i:i + vals_per_line]
            line = ", ".join(f"{v:4d}" for v in row)
            if start + i + vals_per_line < 212:
                header.append(f"    {line},")
            else:
                # Check if this is the very last line
                if end == 212 and i + vals_per_line >= len(chunk):
                    header.append(f"    {line}")
                else:
                    header.append(f"    {line},")
    header.append("};")
    header.append("")
    header.append("#endif // NN_WEIGHTS_H")
    header.append("")

    return "\n".join(header)


def main():
    parser = argparse.ArgumentParser(
        description="Export SenseEdge INT8 weights to C header")
    parser.add_argument("--input", type=str, default=None,
                        help="Input .npz path (default: ml/senseedge_weights.npz)")
    parser.add_argument("--output", type=str, default=None,
                        help="Output .h path (default: firmware/nn_weights.h)")
    args = parser.parse_args()

    ml_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(ml_dir)

    if args.input is None:
        args.input = os.path.join(ml_dir, "senseedge_weights.npz")
    if args.output is None:
        args.output = os.path.join(project_root, "firmware", "nn_weights.h")

    if not os.path.isfile(args.input):
        print(f"ERROR: Weight file not found: {args.input}")
        print("       Run train_senseedge.py first.")
        return 1

    print(f"Loading weights from: {args.input}")
    l1w, l1b, l2w, l2b = load_weights(args.input)

    print(f"Generating C header ...")
    header_text = generate_header(l1w, l1b, l2w, l2b)

    # Ensure output directory exists
    os.makedirs(os.path.dirname(args.output), exist_ok=True)

    with open(args.output, "w") as f:
        f.write(header_text)

    print(f"Wrote: {args.output}")
    print(f"  layer1_weights[128]  (16 x 8,  row-major)")
    print(f"  layer1_biases[16]")
    print(f"  layer2_weights[64]   (4 x 16, row-major)")
    print(f"  layer2_biases[4]")
    print(f"  all_weights[212]     (flat, for sequential loading)")
    print("\nTo load weights into hardware:")
    print("  #include \"nn_weights.h\"")
    print("  #include \"senseedge_regs.h\"")
    print("  for (int i = 0; i < 212; i++)")
    print("      reg_write(SE_NN_WEIGHTS, NN_WEIGHT(i, all_weights[i]));")

    return 0


if __name__ == "__main__":
    sys.exit(main())
